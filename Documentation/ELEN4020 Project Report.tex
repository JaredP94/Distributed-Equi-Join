% witseiepaper-2005.tex
%
%                       Ken Nixon (12 October 2005)
%
%                       Sample Paper for ELEN417/455 2005
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,twocolumn]{witseiepaper}
%
% All KJN's macros and goodies (some shameless borrowing from SPL)
\usepackage{KJN}
\usepackage[super]{nth}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{listings}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{alltt}
%\usepackage{matlab-prettifier}
\usepackage{graphicx}
\usepackage{changes}
\usepackage{makecell}
\usepackage{verbatim}
\usepackage{balance}
\usepackage{pdfpages}
\usepackage{ragged2e}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{multirow}
\usepackage{algpseudocode}
\usepackage{pdfpages}
\usepackage{etoolbox}
\usepackage[super]{nth}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\makeatletter
\patchcmd\@combinedblfloats{\box\@outputbox}{\unvbox\@outputbox}{}{%
	\errmessage{\noexpand\@combinedblfloats could not be patched}%
}%
\makeatother
%\usepackage{flafter}

%\newlength\myindent
%\setlength\myindent{2em}
%\newcommand\bindent{%
%	\begingroup
%	\setlength{\itemindent}{\myindent}
%	\addtolength{\algorithmicident}{\myindent}
%}
%\newcolumntype\eindent{\endgroup}
%
% PDF Info
%
\ifpdf
\pdfinfo{
	/Title (INSTRUCTIONS AND STYLE GUIDELINES FOR THE PREPARATION OF FINAL YEAR LABORATORY PROJECT PAPERS : 2005 VERSION)
	/Author (Ken J Nixon)
	/CreationDate (D:200309251200)
	/ModDate (D:200510121530)
	/Subject (ELEN417/455 Paper Format, 2005)
	/Keywords (ELEN417, ELEN455, paper, instructions, style guidelines, laboratory project)
}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.3mm}} % Defines a new command for the horizontal lines, change thickness here
		
		\center % Center everything on the page
		
		%----------------------------------------------------------------------------------------
		%	HEADING SECTIONS
		%----------------------------------------------------------------------------------------
		\includegraphics[width=0.3\textwidth]{EIE.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
		
		%----------------------------------------------------------------------------------------
		\textsc{\LARGE University of the Witwatersrand } \\[0.1cm] % Name of your university/college
		\textsc{\LARGE School of Electrical and Information Engineering }\\[1cm] % Major heading such as course name
		\textsc{\Large ELEN4020: Data Intensive Computing}\\[1.5cm] % Minor heading such as course title
		
		%----------------------------------------------------------------------------------------
		%	TITLE SECTION
		%----------------------------------------------------------------------------------------
		
		\HRule \\[0.4cm]
		{ \huge \bfseries Project Report} \\[0.4cm] % Title of your document
		\HRule \\[1.5cm]
		
		%----------------------------------------------------------------------------------------
		%	AUTHOR SECTION
		%----------------------------------------------------------------------------------------
		\textsc{\Large 	\emph{Authors:} } \\[0.1cm]	 
		
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				%			\emph{Author:} \\
				Kayla-Jade Butkow \\ 714227 % Your name
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				%	\emph{Author:}\\
				Jared Ping \\ 704447
			\end{flushright}
		\end{minipage}\\[1cm]
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				%		\emph{Author:}\\
				Lara Timm \\ 704157
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				%		\emph{Author:} \\
				Matthew van Rooyen \\ 706692
			\end{flushright}
		\end{minipage}\\[1cm]
		{\large Date Handed In: \nth{14} May, 2018}\\[1cm] 
\vfill	
\justify
\textbf{Abstract: } An equi-join of two large tables using a hash join implemented in parallel using MPI and OpenMP and using MapReduce have been compared and benchmarked against one another. The benchmarks evaluated the running time and memory usage for each approach in the two different algorithms. These algorithms were: . Comparison between results of both algorithms. Note the strengths and weaknesses for each approach.
		
\textbf{Key words: }
		
	\end{titlepage}


\pagestyle{plain}
\setcounter{page}{1}
\twocolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}


This paper compares the performance of two programming models, namely an MPI-OpenMP hybrid and MapReduce, for computing a parallel Equi-Join of two very large tables on their common join attributes. The design and implementation of each model is discussed and a comparative performance analysis between the models is completed. Future recommendations are given based on this discussion.

%Each model will be discussed in terms of its design and implementation; followed by a critical analysis. The performance benchmarks for each of the algorithms will be discussed along with the results if of each benchmark. These results will be comparatively analyzed with future recommendations given.

\section{Project Background}

\subsection{Join Algorithms}

A join operation performs a comparison between two or more relational tables in order to find sets of tuples based on common information, which are concatenated to form a resultant relation~\cite{thomas_zurek_optimisation_1997}.  An equi-join query performs a standard comparison, with the join predicate taking the form of an equals operand (= operator), in order to return tuples which are based on equality between the two tables\cite{joinalg, thomas_zurek_optimisation_1997}. 

To improve the performance and reduce the cost of implementing a relational join on large databases, the computational load can be shared across multiple processors. This is achieved by implementing a parallel join algorithm which reduces the processing time and increases the efficiency of the algorithm for computations on a large scale~\cite{thomas_zurek_optimisation_1997}.

The join operation can be performed in a variety of ways. Two parallel join algorithms are described, namely the sort-merge join and the hash join. %Hashing is used to implement Simple, Grace and Hybrid hash methods where the sort-merge algorithm makes use of sorting and merging. Additionally, the Grace and Hybrid join algorithms first partition the two relations being joined into additional fragments when the inner relation is larger than the amount of available main memory.

\subsubsection{Sort-Merge Join}\label{sortmerge}

The sort-merge join algorithm is based on the principle of sorting both relations according to their join attributes~\cite{thomas_zurek_optimisation_1997}. This is done in order to ensure that, when scanning through both relations, related sets are encountered at similar times. The algorithm consists of two phases, namely the sorting phase and merging phase. For datasets being processed in parallel, the sorting phase occurs by making use of data partitioning~\cite{dist}.

In the first step, each thread partitions the input data. Using range-partitioning allows one to ensure that matching elements in both of the input relations will be processed by the same node\cite{dist}. Within each range the data is sorted and asynchronously sent to it's target node for processing. While the data transfer is taking place, the thread can continue sort the next chunk of input data~\cite{dist}. The performance of this sort algorithm is thus limited by two factors, the rate at which each chunk can be sorted and the network bandwidth available to each process at the given node~\cite{dist}.

After a node has sorted its input data, it waits until it has received all the sorted data pertaining to its range from all of the other nodes~\cite{dist}. The algorithm then merges the sorted chunks into a sorted join result. Multiple iterations over the input data may be needed until both relations are fully sorted~\cite{dist}. After both relations have been sorted they are partitioned among all the nodes for merging. Merging constitutes computing the join result between the relations where the tuples in the output are also sorted~\cite{dist}.

The costs identified with visualizing the output of the sort-merge join depends on the level of differentiation within the input and the transmitted data that needs to be sent across the network\cite{dist}.

\subsubsection{Hash Join}


These joins differ from a sort-merge join in the fact that they contain no initial sorting. Instead, the algorithm has a build phase and a probe phase\cite{equijoin}. 

At the start of the build phase, the algorithm generates a hash table using the smaller relation as the inner join. The hash table contains the join predicate, upon which the hash function is applied, and it's corresponding tuples containing a row of information. The build phase is completed when all the tuples of the initial relation have been stored in the hash table. 

Once the table is created, the larger relation is read in with it's relatives tuples probing the hash table for a corresponding match. This instruction fulfills the probe phase. Searching the hash table for the join predicates is much more efficient than simply scanning through the original relation. 

In order to implement this algorithm in parallel, all threads available are allocated memory at the start of the build phase. The has table is then constructed by creating a join of each thread's piece of memory.The algorithm makes use of whatever memory is available to it and writes the rest to the disk.

The Grace join algorithm differs from that of a simple hash join as it partitions both relations instead of scanning through the second, larger partition. This is achieved by adding a third phase to the algorithm's make up. In the first phase, the algorithm partitions the smaller relation into disk buckets\cite{evaluating4JoinAlgorithms} As is done in the simple hash join, this is achieved by hashing on the join attributes of each tuple within the relation. In phase 2, the larger relation is also partitioned into buckets using the same hash function. In the final phase, the algorithm joins the matching hash table buckets from both relations. This is achieved through using the same hash function on both tables.

The Hybrid has join algorithm exists as an improved method of implementing the Grace join described above. The algorithm is optimized to ensure that it makes use of all available memory on offer from the system. Similar to the Grace algorithm, the Hybrid hash join algorithm also consists of three phases. The hash function used in the first phase to partition the smaller relation ensures that the number of tuples produced for each bucket is small enough to fit within the systems main memory\cite{evaluating4JoinAlgorithms}. The first bucket is then used to create an in-memory hash table while the other buckets produced are temporarily stored in files. The same is done for the larger relation in phase 2, using the same hash function as phase 1. The first bucket of tuples is immediately used to probe the initial in-memory hash table generated in phase 1. It is important to note that the larger relation is divided into the same number of buckets as defined by the small relation. This allows the third phase to the join each bucket in the first relation with it's corresponding bucket in the second. 

The use of main memory in this way means the algorithm requires less I/O operations in comparison to that of a grace join. The joins are also segmented into smaller joins which are implemented for each iteration in the joining phase reducing computation time. When implemented in parallel, the algorithm computation times is decreased as the partitioning of both relations is overlapped with joining of the first buckets from both relations\cite{evaluating4JoinAlgorithms}.

\subsection{OpenMP}

The OpenMP library is a multiprocessing application program inference (API) used to develop shared-memory parallel programs \cite{comparingMPIMapReduce}. The library allows the same parallel code base to be run identically across a range of different operating systems \cite{comparingMPIMapReduce}. OpenMP takes the form of a set of compiler directives (pragmas) which are used for thread creation and synchronization of operations \cite{comparingMPIMapReduce}. 

The pragmas allow for a program to be compiled into a multi-threaded program \cite{kuhn2000openmp}. The OpenMP API makes use of the fork-join parallel design pattern, whereby in the parallel regions, threads are created to perform work concurrently \cite{openMP}. Once the work has been completed, the threads join together to create a single result and to recreate the single control thread \cite{openMP}. All of the threads within a program share a memory address space, thus allowing for efficient communication between the threads \cite{comparingMPIMapReduce}. 

\subsection{Message-Passing Interface}
Message passing is a form of communication between two separate processes \cite{IBM, equijoinWithMPI}. By using this model, processes can send and receive resources \cite{IBM}. If data is transferred, the data must be moved from the local memory of the sending process to that of the receiving process \cite{IBM}.

The Message-Passing Interface (MPI) is a library specification for message passing in parallel and distributed environments \cite{comparingMPIMapReduce}. It is widely used in high-performance computing systems \cite{equijoinWithMPI, joinOnCluster}. The MPI library offers point-to-point communications, broadcast messages and parallel IO \cite{comparingMPIMapReduce}.

MPI can be used for a variety of parallel computing models \cite{comparingMPIMapReduce}. One such model is Single Instruction Multiple Data (SIMD) in which the same instruction is carried out simultaneously on multiple data sets \cite{comparingMPIMapReduce}. Another model is Multiple Instruction Multiple Data (MIMD) in which different instructions are performed on separate sections of data (MIMD)\cite{comparingMPIMapReduce}. The advantages of MPI is that it is a powerful, efficient method of executing parallel programs \cite{IBM}. It is also portable, as message passing is implemented on most parallel platforms \cite{IBM}.

\subsection{MapReduce}

MapReduce is a programming paradigm for processing big data \cite{comparingMPIMapReduce}. The model has been widely adopted for the processing of big data due to it's simplicity and ease of use \cite{comparingMPIMapReduce}. It is thus suitable for solving practical problems \cite{comparingMPIMapReduce, mapReduceJoin}. The algorithm consists of two distinct parts, namely the map function and reduce function \cite{phoenix}. 

The map function takes as an input a line (or chunk) of data, and emits a set of key-value pairs \cite{phoenix}. The map stage can be executed in parallel without any collaboration being required from threads, since each chunk of data is processed independently \cite{comparingMPIMapReduce}. Once the key value pairs have been emitted, the resultant keys are hashed and sorted, and then all values associated with a given key are grouped together \cite{phoenix}. The reduce function then aggregates all the values associated with a key and performs a function that is selected by the programmer \cite{comparingMPIMapReduce}.

The MapReduce model is well suited for data intensive processing for a number of reasons. Firstly, the model is able to sequentially work through the records in a data set without loading the entire set into memory \cite{comparingMPIMapReduce}. The program is also convenient to use as the MapReduce model performs the hashing of the keys and the sorting of the (key,value) pairs inherently \cite{phoenix}. The programmer is able to override these if they wish, but they are not required to, thus enhancing the ease of using the paradigm. Furthermore, the parallel elements are also inherently included in the paradigm, which allows for the programmer to create easily scalable code \cite{comparingMPIMapReduce}. 

Phoenix++ is a C++ based MapReduce implementation that makes use of PThreads to allow for scalability and efficient data processing \cite{phoenix}.

\section{Problem Description}

\subsection{Requirements and Success Criteria}

compare one hybrid programming model and a slightly different algorithmic approach to solving the same problem
performance analysis should be illustrated with speed up and scalability plots
develop and implement a parallel equi-join of two very large relational tables on their common join attributes
output is written to file

\subsection{Assumptions}

During the design and implementation of the hybrid join algorithm the following assumptions were made. 

Performing an equi-join when the join attribute is a non-primary key, and where the chosen attribute is duplicated in one or both of the input relations, results in an output relation containing all possible permutations of tuples in the input for that join attribute. 

Big data constitutes a file size of 1GB or larger. 

\section{System Design and Implementation}
The two implemented solutions are a hybrid equi-join using MPI and OpenMP, and an equi-join implemented using MapReduce. The hybrid solution makes use of the framework set out by the serial solution, but extends it using MPI and OpenMP.

\subsection{Serial Equi-Join}
An object-oriented solution was implemented for the serial equi-join. Within this solution, three classes were created, namely, \texttt{hashFunction}, \texttt{joinManager} and \texttt{fileManager}. The \texttt{fileManager} class is responsible for any functionality relating to the management of files. Within the \texttt{fileManager} class, the input text files are read in and split by delimiter and by end of line character. The data is returned in the form of a vector which contains a key in every even index and the original line from the text file in every odd index.

Once the two input files have been read in and mapped to two vectors, an instance of the the \texttt{hashFunction} class is used to created a hash table. Since a simple hash-join algorithm was implemented, a hash table was only created for one of the input text files \cite{evaluating4JoinAlgorithms}. In creating the table, each key was hashed using \ref{eqn:hash}. By creating a hash of the key, a corresponding (index,(key, value)) pair is created, where the index refers to the location of the value in the hash table. In order to avoid the need for contiguous memory to store the hash table, the table was created as a series of pointers which each point to the next value in the table. If multiple keys are hashed to produce the same index, buckets are created which hold all of the (key,value) pairs with the same hash. The implication of using a hash table is that when searching for a key, the entire table does not need to be searched. Rather, the the key of interest is hashed, and then only the bucket at the corresponding index in the table is searched for the required key.

\begin{equation}
 writeEqnHereOrRatherUseAlgorithm\\
\label{eqn:hash}
\end{equation}

Once the first file has been hashed, the hash table must be probed by the tuples in the second file. To do this, the key is hashed to find its corresponding index in the hash table. Thereafter, each key in the hash table is checked for equality with the probing key. If they are found to be equal (thus satisfying the conditions of an equi-join), the (key,value) pair in the hash table is added to a vector of results. Once the whole bucket has been probed, the vector contains all of the (key, value) pairs that must be joined to the probing key, and it is returned.

Finally, the \texttt{joinManager} class handles the joining of the two tables. In the \texttt{joinManager} class, the value from each resulting (key, value) is appended onto the probing (key, value) pair in order to join the tables. To reduce repetition of the key in the joined table, the key is extracted from the resulting value prior to appending the strings. The joined strings are then appended to a vector. This process occurs for each (key,value) pair in the second file. Finally, the vector containing the joined table is returned and written to an output text file.

\subsection{Hybrid Equi-Join}
The first implemented method utilises the MPI communications framework in conjunction with the OpenMP API. The MPI framework is used to create a scalable solution which can run on clusters of any size, allowing each node within the cluster to contribute to the computational capability of executing the program \cite{mpi-scale}. The mpic++ compiler is utilised which provides a wrapper that interfaces with the g++ compiler \cite{mpic++-wrapper}. A 64~bit version of the g++ compiler is utilised thus inferring that file buffer pointers are now 64~bit pointers \cite{pointer-size}. By considering that the file buffer is a \texttt{signed long int} data type, this translates to the ability to parse files up to 8.5~EB in size. Thus the memory bottleneck will most likely be enforced by the amount of RAM and page memory available within the cluster. This allows the file content to be read in completely, minimising data access overhead that could be incurred by reading in data chunks through the program execution.

OpenMP was selected for the hybrid model since it makes the implementation of mulithreaded programs much simpler, since the complier transforms the serial code into parallel according to the directives \cite{comparingMPIMapReduce}.

A \texttt{FileManager} object is utilised by the master node to read in the two data files that are required to be joined. The contents of the smaller file is sent to each slave node and a hash table is created on each slave node. When performing the has, the value at the specified column index is set as the key and the line from which it was extracted is set as the value.

The larger file is divided up into chunks, whereby the size of each chunk is determined by the number of slave nodes available for computation within the cluster. The divided chunks are then distributed between the slave nodes to allow them to perform the equi-join on the data segments they have access to.

Node communication is made possible through the use of the \texttt{MPI\_send} and \texttt{MPI\_recv} functions \cite{mpi-send}. The \texttt{MPI\_send} function specifies the slave node which data is being sent to. The number of expected incoming variables must be sent before the variable(s) itself is sent. The data chunk intended for each slave node is sent in the order of their node ranks. Each corresponding pair of sent data is then received by the slave nodes as each process iterates through the program and executes the corresponding \texttt{MPI\_recv} function pairs. The order of these receives must correspond to the order of the sends initiated by the master node \cite{mpi-send}. Failure to implement this execution flow correctly will lead to slave nodes receiving incorrect or incomplete data segments.

Each slave node utilises the OpenMP API to parallelise the hashing of the two received data segments. Computation on a single thread is required when the \texttt{join} function of the \texttt{hashFunction} object is run. This is incurred due to the function utilising a \texttt{std::vector} container from the C++ Standard Template Library (STL) which does not allow concurrent modification operations \cite{stl-vector}. This presents a tradeoff in the design, which is discussed in detail in \ref{sec:tradeoffs}.

Once each slave node has completed its required hashing computations the resulting joined data is sent back to the master node. The order of received data is not important and thus the transmitted results from slave nodes can be received as it is completed. The master then proceeds to write the resultant data to an output file as each resultant data chunk is received. This write process occurs on a single thread to prevent data corruption due to simultaneous writes to a single output file.

\subsection{MapReduce Equi-Join}
The second method implemented makes use of the Phoenix++ MapReduce framework. MapReduce was selected since it is the industry standard for the processing of large amounts of data \cite{comparingMPIMapReduce}. The implemented algorithm is a general reducer-side join \cite{mapReduceJoin}. Within this algorithm, a single map and reduce stage is used. Within the map stage, an identifier corresponding to each file is attached to the value \cite{mapReduceJoin}. The (key, value) pair is then emitted. Within the reduce stage, data with the same key and a different tag are joined together \cite{mapReduceJoin}.

In order to compensate for input text files that are too large to be held in RAM all at once, the input text files are mapped to memory. They can thus be read in when needed, in chunks that can be held in RAM. Since the memory map maps the data directly back to the text file, in order to edit the data, the file is also edited. To avoid altering the input text file, two copies of each file are made - one used for isolating the key, and one for the value. Thus, four input files are mapped to memory. This presents a tradeoff in the design, which is discussed in detail in \ref{sec:tradeoffs}.

After mapping the data to memory, the data is divided into chunks to be sent to the map processes. Checks are performed to ensure that each chunk only contains full lines of the text file. This is essential in ensuring that none of the (key, value) pairs are missed.

Within the mapper, the key value pairs from the text file need to be isolated and emitted. In order to isolate the values, the file is iterated through until an end of line character is encountered. This end of line character is set to be the integer \texttt{0}, which is the integer value of the null terminator, and indicates where a string ends \cite{phoenix}. This string represents the value.

To isolate the keys, the column that contains the keys must be known. This information is obtained from the program's run-time arguments. When iterating through the text files, a check is done to determine whether each character is a delimiter or a normal text character. If the character is a delimiter, a counter of delimiters is incremented. When the delimiter counter is equal to the number of the column containing the keys, the start of the key is recorded as the following character. The string is then iterated through to find the next delimiter which marks the end of the key. This character is then set to \texttt{0}. The key and the value are then emitted to the reducer. In order to identify which (key, value) pair originated from which input file, the pair is also emitted with an identifier (\texttt{0} for file 1 and \texttt{1} for file 2).

Once the mapping process is completed, the keys are hashed and the data is sorted. These processes were however not overwritten to implement the join and as such, the in-built Phoenix++ functionality was used.

Each key, and all of its corresponding values, is then sent to the reducer. Within the reducer, the values are split into two vectors based on their identifiers. Thereafter, for each combination of the two vectors, a string is created which contains the two values appended together. The process of appending the two values is the joining of the row of the two tables. Together with the key, the joined value is emitted from the reducer, and written to a text file.

Finally, to complete the program, the files are unmapped from memory and the four copies input files are deleted.

Phoenix++ inherently makes use of PThreads and as such, the equi-join is performed in parallel.

%\subsection{File System Design}
%
%\subsection{Code Framework}
%
%The algorithm was constructed using C++ and Message Passing Interface library (MPI). MPI libraries consist of message-passing functionality which was used for distributed and parallel algorithms. Message passing function is simply a func- tion that explicitly transmits data from one process to another. By using MPI library each process that is running on an individual workstation is assigned a unique number called process rank. The root process will coordinate most of the operation for the parallel data joining implementation. The equi-join employed the data parallelism paradigm. Data is partitioned to different process for local processing. The main MPI library functions used in our implementation are Send, Receive, Broadcast, Gather, and All-to-All communication
%
%\subsection{Join Algorithm}
%
%Parallel collection equi-join can be divided into two main phases. The first phase is data partitioning based on disjoint partition, and the second phase is per- forming the join algorithm(i.e. sort-merge or hash join). The hash join algorithm was chosen for this.
%
%\subsection{Multi-threading Environment}
%
%OpenMP. Each node has 4 cores, 8 threads. 
%
%
%Compared to using pthreads and working with mutex and
%condition variables, OpenMP is much easier to use because
%the compiler takes care of transforming the sequential code
%into parallel code according to the directives [12]. Hence
%the programmers can write multithreaded programs without
%serious understanding of multithreading mechanism. Its
%runtime maintains the thread pool and provides a set of
%libraries [7].
%It uses a block-structured approach to switch between
%sequential and parallel sections, which follows the fork/join
%model. At the entry of a parallel block, a single thread of
%control is split into some number of threads, and a new
%sequential thread is started when all the split threads have
%finished. Its directives allow the fine-grained control over
%the threads. It is supported on various platforms like UNIX,
%LINUX, and Windows and various languages like C, C++,
%and Fortran [12].
%
%
%In OODB, collection equi-join is based on ’total’ equality. That is, for list/array each collection type element (OID) must be equal in the same order. Whereas for set/bag two objects are equal if all collection type element object of class A exist in object of class B in any order. Before the joining can take place, the local objects for class A and B needs to be sorted. We used quick sort with algorithm in Figure 3 as the comparison function. The algorithm performs element-by-element comparison between two collec- tion attributes until one of the OIDs is not equal or it reaches the end of one of the collection attributes. For this comparison to perform correctly, collection attributes of type set or bag must be pre-sorted

\section{Results}
To test the performance of the two solutions, the solutions were run with different sized files. These files were generated such that each file contained the same keys. As such, each line of the two input files was joined in the output file. For the Hybrid solution, the number of nodes was altered, and for the MapReduce solution, the number of threads was altered.

In table \tabref{tab:results2}, each input file had 11~881~376 lines with a size of 1.01~GB. The input files were created such that each key in file 1 had a match in file 2.

\begin{table*} [t]
	\centering
	\caption{Comparison of time taken to compute the equi-join using MapReduce with two 1~GB files over various numbers of threads}
	\label{tab:results2}
	
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		Number of threads & 1 & 4 & 8 &16\\ 
		\hline
		\hline
		Time For Map Reduce (s) & 44.206261 & 14.993695 & 14.163257 &20.268396\\ 
		\hline 
		Time For File Write (s) & 13.972962 & 15.215831 & 13.157052 &16.377010\\ 
		\hline 
		Total Time (s) & 58.179223 & 30.209526 & 27.320309 &  36.645406\\ 
		\hline 
	\end{tabular} 
\end{table*}

In table \tabref{tab:results3}, each input file had 24~137~569  lines with a size of 2.00~GB. 

\begin{table*} [t]
	\centering
	\caption{Comparison of time taken to compute the equi-join using MapReduce with two 2~GB files over various numbers of threads}
	\label{tab:results3}
	
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		Number of threads & 1 & 4 & 8 &16\\ 
		\hline
		\hline
		Time For Map Reduce (s) & 93.469804 & 31.426143 & 30.058178 & 41.858639\\ 
		\hline 
		Time For File Write (s) & 28.433646 & 32.931398& 24.158987 & 24.892521\\ 
		\hline 
		Total Time (s) & 121.903450 & 64.357541 & 54.217165 & 66.751160 \\ 
		\hline 
	\end{tabular} 
\end{table*}


\section{Critical Analysis}

*****Put speed up and scalability graphs here!******


A strength of the designed solutions is that both solutions are able to perform equi-join operations on any column of a table. Furthermore, the hybrid solution is able to handle any type of delimiter within a file.

\subsection{Limitations}
A limitation of the implemented MapReduce model is that an assumption is made that the reducer has sufficient memory to hold all of the values with the same key \cite{mapReduceJoin}. If the reducer does not have sufficient memory, the performance of the algorithm is greatly affected.

\subsection{Tradeoffs} \label{sec:tradeoffs}
The use of a STL \texttt{std::vector} to store the hash join results is a large tradeoff of the system. While this usage allows for reduced computational complexity and improved platform compatibility, it creates redundancies in the efforts to parallelise all possible components of the hybrid equi-join algorithm. Therefore this design choice creates a large inefficiency within the system.

The use of memory mapping to store the input text files for the MapReduce solution is a large tradeoff of the system. While this usage means that larger input files can be processed, it also lead to the need to duplicate each input text file twice, and allocate memory for each of the four text files. This creates a large inefficiency within the system.

\section{Future Recommendations}
For future recommendations, a hybrid hash-join algorithm should be implemented rather than a simple hash-join. This would improve performance at all levels of memory availability \cite{evaluating4JoinAlgorithms}.

Furthermore, the performance of the hybrid equi-join algorithm could be greatly improved through the use of a concurrent vector, such as the Thread Building Blocks (TBB) \texttt{tbb::concurrent\_vector}, or by utilising the \texttt{std::mutex} class to manage thread synchronisation of modification operations to the STL \texttt{std::vector} \cite{tbb} \cite{mutex}.

\section{Conclusion}

\bibliographystyle{witseie}
\bibliography{bigData}

\end{document}